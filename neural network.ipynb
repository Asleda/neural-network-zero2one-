{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络——随机梯度下降and反向传播\n",
    "## Abstract\n",
    "> 神经网络是当前比较重要的算法，它模拟了人类大脑神经元突触传播方式，在一定程度上提高了当前的识别任务的精度。这篇文章对基本的全连接神经网络进行了分析，运用最小批随机梯度下降和反向传播来更新层间权重$w$和偏差$b$，最小化目标函数。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "> 神经网络是由参数决定的，随着参数的增加它几乎可以拟合任何函数，所以它的效果非常好。早期神经网络难以进行训练，后来出现的反向传播方法很好的改善了这种情况。在这篇文章中，我们运用反向传播算法训练一个识别手写体数字的神经网络，算法没有经过任何改进，是最基本的形式，这有助于我们理解神经网络中发生了什么。文章组织如下，2部分讨论数据，3中简绍一般网络的结构，目标函数的设定在4中叙述，5中讨论了激活函数，6部分讨论随机梯度下降算法，最后在7中我们将会推导方向传播算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data\n",
    "> 机器学习算法需要数据，并且是大量的数据，只有和数据结合起来算法才有实际意义。Neural network 是有监督学习，它需要一定量带有类别标签的数据。一定程度上可以说算法的参数是由训练数据决定的，数据进入网络后，相关性出现波动，数据要经历压缩代表最终产生可靠的结果。这里我们使用的NIST发布的手写体数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Structure\n",
    "> 我们一般把第一层叫做输入层，最后一层叫做输出层，所有中间的都叫做隐层。隐层的数量和神经元的个数都是非常重要的。不同的任务需要设计不同网络结构，也就是决定层数和其中神经元的个数。之后我们会进一步讨论其他的网络的结构，有些网络比如RNN,CNN,GAN等，他们各有专长。我们这里就仅仅采用了全链接的神经网络，层数和神经元个数不定。可以改变网络结构观察一下效果变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 目标函数\n",
    "> 目标函数是算法要优化的目标。一般是会根据运用场景和数据来构造合适的目标函数（高级水平），目标函数貌似也决定了网络的性能，一般情况下使用常用的目标函数如，均方差等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 激活函数\n",
    "> 激活函数使得网络学习成为了可能，并且也牵扯到学习效率的问题。不同的激活函数收敛速度应该也不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. $w$ and $b$\n",
    "> 说白了，神经网络主要的工作就是要求出所有的$w$和$b$。不管使用什么数据、什么样的网络结构、构造什么样的目标函数、选择什么激活函数，我们的目标就是要求出使得目标函数最小的$w$和$b$。也就是说数据、网络结构、目标函数、激活函数还有$w$和$b$构成了神经网络最基本的样子。下面我们将会讨论一下梯度下降和反向传播算法，这两个算法就是用来求参数$w$和$b$的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 梯度下降\n",
    "> 这里我们使用的目标函数是$C(w,b) = \\frac{1}{2n} \\sum_x \\| y(x) - a\\|^2$,我们的目标是要找到$w$和$b$使得目标函数尽可能的小。\n",
    "在推到的时候我们把目标函数看为抽象的$C(v)$,其中$v$代表着参数，可以认为$v=v_1,v_2, \\dots,v_n$，这样的有助于我们脱离某个具体函数的限制，这也意味着目标函数不同时，我们的方法仍然可以适用。\n",
    "\n",
    "> 设想，当$w$或者$b$ 发生改变的时候，势必传播到目标函数，这是有两种情况，要么减小要么增大，\n",
    "\\begin{eqnarray} \n",
    "  \\Delta C \\approx \\frac{\\partial C}{\\partial v_1} \\Delta v_1 +\n",
    "  \\frac{\\partial C}{\\partial v_2} \\Delta v_2.\n",
    "\\end{eqnarray}\n",
    "假如，我们让$\\Delta C$ 为负数，那就可以保持目标函数一直下降 。这里把他们看成向量相乘$\\Delta C \\approx \\nabla C \\cdot \\Delta v$,右式两个因子，我们可以选择的是$\\Delta v$ 也就是可以决定参数$v$要向哪里变化，为了令$\\Delta C<=0$,这里令$\\Delta v = -\\eta \\nabla C$。\n",
    "\n",
    ">找到$v$的位移$\\Delta v = -\\eta \\nabla C$, 把这联系到$w$和$b$也是一样的，所以可以得到\n",
    "\\begin{eqnarray}\n",
    "  w_k & \\rightarrow & w_k' = w_k-\\eta \\frac{\\partial C}{\\partial w_k} \\\\\n",
    "  b_l & \\rightarrow & b_l' = b_l-\\eta \\frac{\\partial C}{\\partial b_l}.\n",
    "\\end{eqnarray}\n",
    ">这时我们找到了$w$和$b$的更新策略，但是还有一个重点，就是如何计算$\\partial w$和$\\partial b$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.反向传播算法\n",
    ">反向传播算法使得神经网络的训练成为现实，能够求出每层的$\\partial w$和$\\partial b$。\n",
    "假如目标函数的值比较大时，说明某些 $w$,$b$ 还需要进行调整。如果设定理想化的$w$和$b$是正确的参数，呢在未达到理想化的时候，网络要不停的改变错误。在全链接的情况下，网络的错误是要所有神经元来弥补的。也可以说，当网络有错误时，错误反向传播经过前面的一层层，然后在每层上减少错误（通过调整神经元的输出）。\n",
    "\n",
    ">定义 $z^l=w^l \\cdot a^{l-1}+b^l$，我们偏导直接作用在$z$上，\n",
    "$$\\frac{\\partial C}{\\partial Z^l}\\cdot \\Delta Z^l+\\frac{\\partial C}{\\partial Z^{l+1}}\\cdot \\Delta Z^{l+1}+\\dots+\\frac{\\partial C}{\\partial Z^n}\\cdot \\Delta Z^n=\\Delta C$$\n",
    "在未达到理想化时，$\\Delta C$ 就是网络的错误总量，可以从式子中看出来，每个神经元都在努力改进由自身产生的错误。如此，对于 $\\frac{\\partial C}{\\partial Z^l}\\cdot \\Delta Z^l$, 神经元在看到错误的时候做出的努力是 $\\Delta Z^l$, 为了中和掉 $\\frac{\\partial C}{\\partial Z^l}$, 所以$\\frac{\\partial C}{\\partial Z^l}$ 就是l层的错误。需要说明的是选用偏导直接作用在$Z$上，而不是直观的作用在激活函数上，这是因为本质上错误是通过调整$w$和$b$而优化的， 调整$z^l=w^l \\cdot a^{l-1}+b^l$ 比$\\sigma(z)$ 更精准也更方便。按照这种方式，链式求导法则可以求出每层的错误，当网络的层数较少时完全可以应对，但随着层数增加链式求导要反复的重复计算，效率极低。可以这样思考，我们计算时，数据是前向处理传播到最后一层，在减小误差时，错误可不可以反向传播到前一层？答案是肯定的，并且我们猜测应该还要借助层间$w$来实现。\n",
    "\n",
    "\n",
    ">定义层间误差，\n",
    "$$\n",
    "\\delta^l = \\frac{\\partial C}{\\partial z^l}\n",
    "$$\n",
    "其中求导过程为\n",
    "\n",
    "\\begin{eqnarray}\n",
    " \\delta^l & = & \\frac{\\partial C}{\\partial a^l}\\cdot \\frac{\\partial a^l}{\\partial z^l}\\\\\n",
    " & = & \\frac{\\partial C}{\\partial a^l} \\cdot \\sigma'(z)\n",
    "\\end{eqnarray}\n",
    "我们现在要和上一层的误差连起来，\n",
    "\\begin{eqnarray}\n",
    "  \\delta^l & = & \\frac{\\partial C}{\\partial z^l} \\\\\n",
    "  & = & \\frac{\\partial C}{\\partial z^{l+1}} \\frac{\\partial z^{l+1}}{\\partial z^l} \\\\ \n",
    "  & = &  \\frac{\\partial z^{l+1}}{\\partial z^l} \\delta^{l+1}\n",
    "\\end{eqnarray}\n",
    "其中$ z^{l+1}=  w^{l+1} a^l +b^{l+1} =  w^{l+1} \\sigma(z^l) +b^{l+1}$, 所以 $\\frac{\\partial z^{l+1}}{\\partial z^l} = w^{l+1}\\sigma'(z^l)$。\n",
    "到这里我们就得到层间误差的关系，\n",
    "$$\n",
    " \\delta^l =  w^{l+1} \\cdot \\delta^{l+1}\\cdot \\sigma'(z^l)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ">根据上式，当我们求得最后一层的误差，我们就可以以此得到前面所有层的误差。但是，我们最初的目的是要求$ \\frac{\\partial C}{\\partial w}$和$ \\frac{\\partial C}{\\partial b}$, 此时要把他们和$\\delta^l$联系起来。做一个链式求导就可以了，\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w^l}=\\frac{\\partial C}{\\partial z^l} \\cdot \\frac{\\partial z^l}{\\partial w^l}\\\\\n",
    " =\\delta^l \\cdot \\frac{\\partial z^l}{\\partial w^l}\\\\\n",
    " =\\delta^l \\cdot \\sigma(z^{l-1})\n",
    "$$\n",
    "> 接下来，\n",
    "\\begin{eqnarray}\n",
    "  \\frac{\\partial C}{\\partial b^l}=\\frac{\\partial C}{\\partial z^l} \\cdot \\frac{\\partial z^l}{\\partial b^l}\\\\\n",
    "  =\\delta^l \\cdot 1\\\\\n",
    "  =\\delta^l\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.总结\n",
    "\n",
    "> 反向传播中，我们首先找到每层中的误差$\\delta$，然后把每层的误差和$\\frac{\\partial C}{\\partial w}$, $\\frac{\\partial C}{\\partial b}$ 联系起来。最后运用随机梯度下降，然后就可以根据训练数据不断更新$w$和$b$。\n",
    "但是在现实中应用中，情况并不是如此简单乐观。比如这里我们要对随机梯度下降改为最小批随即梯度下降。下面是详细的代码实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mnist_loader 是加载数据的脚本\n",
    "这里加载了训练数据，验证数据，测试数据。其中training_data是进行训练的数据，validation_data是要利用其进行参数调整，test_data是测试模型泛化能力\n",
    "的数据集。\n",
    "这里使用的数据集是由NIST整理的mnist手写体数据集。\n",
    "\"\"\"\n",
    "import mnist_loader\n",
    "training_data,validation_data,test_data=mnist_loader.load_data_wrapper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "这里定义激活函数，在上面的推到中每层的误差中也要用到激活函数的倒数\n",
    "所以这里定义两个函数\n",
    "\"\"\"\n",
    "\n",
    "#sigmoid 函数\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "#sigmoid 函数的倒数\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1.0-sigmoid(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self,sizes):\n",
    "        #初始化网络结构,定义网络的层数和每层中神经元的个数，并初始化w,b\n",
    "        self.num_layer=len(sizes)\n",
    "        self.sizes=sizes\n",
    "        self.biases=[np.random.randn(y,1) for y in sizes[1:]]\n",
    "        self.weights=[np.random.randn(x,y) for x,y in zip(sizes[1:],sizes[:-1])]\n",
    "        \n",
    "    def feedforward(self,a):\n",
    "        #前向传播，神经网络的计算方式。当a输入，产生output.\n",
    "        for w,b in zip(self.weights,self.biases):\n",
    "            a=sigmoid(np.dot(w,a)+b)\n",
    "        return a \n",
    "    \n",
    "    def SGD(self,training_data,epochs,mini_batch_size,eta,test_data=None):\n",
    "        \"\"\"使用最小批随即梯度下降，以批为单位调整w,b,epoches是训练周期数，mini_batch_size是批的大小\n",
    "           eta是学习率 \"\"\"\n",
    "        if test_data: n_test=len(test_data)\n",
    "        n=len(training_data)\n",
    "        for i in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches=[training_data[ep:ep+mini_batch_size] for ep in range(0,n,mini_batch_size) ]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch,eta)\n",
    "            if test_data:\n",
    "                print(\"Epochs{0}:{1}/{2}\".format(i,self.evaluate(test_data),n_test))\n",
    "            else:\n",
    "                print (\"Epochs{0} is complete\".format(i))\n",
    "        \n",
    "    def update_mini_batch(self,mini_batch,eta):\n",
    "        \"\"\"最小批是把批中每个数据更新的w,b全部加起来取均值\n",
    "        \"\"\"\n",
    "        partial_b=[np.zeros(b.shape) for b in self.biases]\n",
    "        partial_w=[np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        for x,y in mini_batch:\n",
    "            new_partial_b,new_partial_w=self.backprop(x,y)\n",
    "            \n",
    "            partial_b=[pb+npb for pb,npb in zip(partial_b,new_partial_b)]\n",
    "            partial_w=[pw+npw for pw,npw in zip(partial_w,new_partial_w)]\n",
    "            \n",
    "        self.weights=[w-(eta/len(mini_batch))*nw for w,nw in zip(self.weights,partial_w)]\n",
    "        self.biases=[b-(eta/len(mini_batch))*nb for b,nb in zip(self.biases,partial_b)]\n",
    "    \n",
    "    def backprop(self,x,y):\n",
    "        partial_b=[np.zeros(b.shape) for b in self.biases]\n",
    "        partial_w=[np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        \n",
    "        activations=[x]\n",
    "        zs=[]\n",
    "        for w,b in zip(self.weights,self.biases):\n",
    "            z=np.dot(w,x)+b\n",
    "            zs.append(z)\n",
    "            a=sigmoid(z)\n",
    "            x=a\n",
    "            activations.append(a)\n",
    "        #目标函数不同，delta不同,这是网络最后一层的误差\n",
    "        delta=(activations[-1]-y)*sigmoid_prime(zs[-1])\n",
    "        #知道层的误差之后，可以求出偏w,偏b\n",
    "        partial_b[-1]=delta\n",
    "        partial_w[-1]=np.dot(delta,activations[-2].transpose())#这里的转置是要保证w.shape不变\n",
    "        \n",
    "        #下面我们要进行反向传播 \n",
    "        for ly in range(2,self.num_layer):\n",
    "            z=zs[-ly]\n",
    "            sp=sigmoid_prime(z)\n",
    "            delta=np.dot(self.weights[-ly+1].transpose(),delta)*sigmoid_prime(z)\n",
    "            partial_b[-ly]=delta\n",
    "            partial_w[-ly]=np.dot(delta,activations[-ly-1].transpose())\n",
    "        return partial_b,partial_w\n",
    "        \n",
    "    def evaluate(self,test_data):\n",
    "        test_results=[(np.argmax(self.feedforward(x)),y)for x,y in test_data]\n",
    "        return sum(int(x==y) for x,y in test_results)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs0:4401/10000\n",
      "Epochs1:4224/10000\n",
      "Epochs2:5147/10000\n",
      "Epochs3:5162/10000\n",
      "Epochs4:5299/10000\n",
      "Epochs5:5497/10000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"实例化一个网络\"\"\"\n",
    "net=Network([784,25,10])\n",
    "net.SGD(training_data,30,10,3.0,test_data=test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
